# -*- coding: utf-8 -*-
"""transition generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hbji0Kn2IrT3a_r6SPLAgmpctq3sPOeP
"""

# Commented out IPython magic to ensure Python compatibility.
# Clone AnimateDiff repo
!git clone https://github.com/Kyotogod/AnimateDiff-Colab.git
# %cd AnimateDiff-Colab

# Install dependencies
!pip install -r requirements.txt

# Download AnimateDiff motion module (already included in notebook below, optional here)
!mkdir -p models/Motion_Module
!wget -P models/Motion_Module https://huggingface.co/guoyww/AnimateDiff/resolve/main/mm_sd_v15_v2.ckpt

# Download Stable Diffusion 1.5 model (anime one recommended)
!mkdir -p models/StableDiffusion
!wget -P models/StableDiffusion https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/model.ckpt

# Install ffmpeg if not already installed
!apt-get -y install ffmpeg

# Create the transition video
!ffmpeg -framerate 12 -i frames/frame_%03d.png -c:v libx264 -pix_fmt yuv420p transition.mp4

from PIL import Image
import os
start_img_path = "/content/panel.jpg"
end_img_path = "/content/end.png"
start_img = Image.open(start_img_path).resize((512, 512))
end_img = Image.open(end_img_path).resize((512, 512))

def interpolate_images(img1, img2, steps):
    img_seq = []
    for i in range(steps):
        alpha = i / (steps - 1)
        blended = Image.blend(img1, img2, alpha)
        img_seq.append(blended)
    return img_seq

frames = interpolate_images(start_img, end_img, steps=32)

# Save frames
os.makedirs("frames", exist_ok=True)
for i, frame in enumerate(frames):
    frame.save(f"frames/frame_{i:03d}.png")

!ffmpeg -framerate 32 -i frames/frame_%03d.png -c:v libx264 -pix_fmt yuv420p transition.mp4

!pip install diffusers transformers accelerate ftfy
!pip install git+https://github.com/openai/CLIP.git
!pip install --upgrade torch torchvision torchaudio

from diffusers import StableDiffusionPipeline
import torch
from PIL import Image
import numpy as np
import os

# Load SD1.5 from HuggingFace
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
).to("cuda")

pipe.enable_attention_slicing()

def image_to_latent(image_path):
    image = Image.open(image_path).convert("RGB").resize((512, 512))
    image = pipe.feature_extractor(images=image, return_tensors="pt").pixel_values.to("cuda", dtype=torch.float16)
    latents = pipe.vae.encode(image).latent_dist.sample() * 0.2
    return latents

latent_start = image_to_latent("/content/panel.jpg")
latent_end = image_to_latent("/content/end.png")

def interpolate_latents(start, end, steps=32):
    return [(1 - t) * start + t * end for t in np.linspace(0, 1, steps)]

latent_seq = interpolate_latents(latent_start, latent_end, steps=32)

def latent_to_image(latent):
    latent = latent / 0.18215
    image = pipe.vae.decode(latent).sample
    image = (image / 2 + 0.5).clamp(0, 1)
    # image = image.cpu().permute(0, 2, 3, 1).numpy()[0]
    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()[0]

    return (image * 255).astype(np.uint8)

os.makedirs("transition_frames", exist_ok=True)

for i, latent in enumerate(latent_seq):
    img = latent_to_image(latent)
    Image.fromarray(img).save(f"transition_frames/frame_{i:03d}.png")

!rm -rf /content/frames

!ffmpeg -framerate 16 -i transition_frames/frame_%03d.png -c:v libx264 -pix_fmt yuv420p transition2.mp4

# !pip install git+https://github.com/facebookresearch/dinov2.git
!pip install timm
# This combo works well on Colab with GPU runtime
# ‚úÖ Reset everything to working versions
# !pip uninstall -y torch torchvision torchaudio numpy
!pip install numpy==1.24.3
!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118

!pip install pillow

# import torch
from torchvision import transforms
# from PIL import Image
# from dinov2.models import vision_transformer as vits
# from dinov2.data.transforms import make_classification_eval_transform

# # Load DINOv2 base model
# model = vits.vit_base(patch_size=14)
# model.load_state_dict(torch.hub.load_state_dict_from_url(
#     "https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14_pretrain.pth",
#     map_location="cpu"
# ))

# model.eval().cuda()
import torch

# Load the ViT-B/14 DINOv2 model
dinov2_vitb14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')
model = dinov2_vitb14
model.eval().cuda()
# If you need the model with registers, use:
# dinov2_vitb14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14_reg')

from PIL import Image
def preprocess(image_path):
    img = Image.open(image_path).convert("RGB")
    transform = transforms.Compose([
        transforms.Resize((518, 518)),  # DINOv2 works best around 518x518
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)
    ])
    return transform(img).unsqueeze(0).cuda()

def get_dino_features(image_tensor):
    with torch.no_grad():
        feats = model.forward_features(image_tensor)
        return feats["x_norm_clstoken"]  # CLS token

# img1 = preprocess("panel.jpg")     # Your manga image
# img2 = preprocess("frame.png")     # Anime frame

feat1 = get_dino_features(img1)
feat2 = get_dino_features(img2)

# Cosine similarity
similarity = torch.nn.functional.cosine_similarity(feat1, feat2).item()
print(f"üß† DINOv2 Cosine Similarity Score: {similarity:.4f}")

!rm -rf /content/frames
import os
os.makedirs("frames", exist_ok=True)

!ffmpeg -i punch.mp4 -vf fps=4 frames/frame_%04d.png

!pip install ftfy regex tqdm
!pip install git+https://github.com/openai/CLIP.git

import clip
import torch
from PIL import Image

# device = "cuda" if torch.cuda.is_available() else "cpu"
device = "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# Load & preprocess images
img1 = preprocess(Image.open("panel.jpg")).unsqueeze(0).to(device)
img2 = preprocess(Image.open("frame.png")).unsqueeze(0).to(device)

with torch.no_grad():
    feat1 = model.encode_image(img1)
    feat2 = model.encode_image(img2)

    similarity = torch.nn.functional.cosine_similarity(feat1, feat2).item()

print(f"üß† CLIP Cosine Similarity Score: {similarity:.4f}")

!pip install open-clip-torch

import open_clip
import torch
from PIL import Image
from torchvision import transforms

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load pretrained OpenCLIP model (ViT-B/32 trained on LAION-2B)
model, _, preprocess = open_clip.create_model_and_transforms(
    'ViT-B-32', pretrained='laion2b_s34b_b79k'
)
model.to(device)
model.eval()

def load_image(path):
    img = Image.open(path).convert("RGB")
    return preprocess(img).unsqueeze(0).to(device)

# Load your images
img1 = load_image("panel.jpg")
img2 = load_image("frame.png")

# Encode with OpenCLIP
with torch.no_grad():
    feat1 = model.encode_image(img1)
    feat2 = model.encode_image(img2)
    similarity = torch.nn.functional.cosine_similarity(feat1, feat2).item()

print(f"üîç OpenCLIP Cosine Similarity Score: {similarity:.4f}")

def load_image_grayscale(path):
    img = Image.open(path).convert("L")  # Convert to grayscale
    img = img.convert("RGB")             # Convert back to RGB (for model input shape)
    return preprocess(img).unsqueeze(0).to(device)

img1 = load_image("panel.jpg")             # manga panel
img2 = load_image_grayscale("frame.png")   # anime frame (now B&W)

with torch.no_grad():
    feat1 = model.encode_image(img1)
    feat2 = model.encode_image(img2)
    similarity = torch.nn.functional.cosine_similarity(feat1, feat2).item()

print(f"üñ§ Grayscale OpenCLIP Similarity Score: {similarity:.4f}")

# !pip install ftfy regex tqdm
# !pip install git+https://github.com/openai/CLIP.git
import clip
import torch
from PIL import Image
import os
from torchvision import transforms

device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)
query_image = preprocess(Image.open("panel.jpg").convert("RGB")).unsqueeze(0).to(device)
with torch.no_grad():
    query_embedding = model.encode_image(query_image)

import glob

frame_folder = "frames"  # or "/content/drive/MyDrive/your_folder"
frame_files = sorted(glob.glob(f"{frame_folder}/*.png"))
top_matches = []

for f in frame_files:
    try:
        frame_img = preprocess(Image.open(f).convert("RGB")).unsqueeze(0).to(device)
        with torch.no_grad():
            frame_embed = model.encode_image(frame_img)
            sim = torch.nn.functional.cosine_similarity(query_embedding, frame_embed).item()
            top_matches.append((f, sim))
    except:
        continue

top_matches.sort(key=lambda x: -x[1])  # Highest score first

# Show top 3
for fname, score in top_matches[:3]:
    print(f"‚úÖ Frame: {fname}, Similarity: {score:.4f}")

from IPython.display import display

print("üéØ Best Match:")
display(Image.open(top_matches[0][0]))

from IPython.display import display
from PIL import Image

print("üéØ Top 10 Matches:\n")

for i, (fname, score) in enumerate(top_matches[:10]):
    print(f"{i+1}. Frame: {fname}, Similarity: {score:.4f}")
    display(Image.open(fname))

# !pip install git+https://github.com/facebookresearch/dinov2.git
# !pip install timm
import torch
from PIL import Image
from torchvision import transforms
# from dinov2.models import vision_transformer as vits

# Load the ViT-B/14 DINOv2 model

import os, glob

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load DINOv2 Base
# model = vits.vit_base()
# state_dict = torch.hub.load_state_dict_from_url(
#     "https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14_pretrain.pth",
#     map_location=device
# )
# model.load_state_dict(state_dict)
# model.eval().to(device)

# DINOv2 transform
dinov2_vitb14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')
model = dinov2_vitb14
model.eval().cuda()

dino_preprocess = transforms.Compose([
    transforms.Resize((518, 518)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])
query_img = dino_preprocess(Image.open("panel.jpg").convert("RGB")).unsqueeze(0).to(device)

with torch.no_grad():
    query_feat = model.forward_features(query_img)["x_norm_clstoken"]
frame_dir = "frames"  # folder with extracted video frames
frame_files = sorted(glob.glob(f"{frame_dir}/*.png"))

results = []

for path in frame_files:
    try:
        img = dino_preprocess(Image.open(path).convert("RGB")).unsqueeze(0).to(device)
        with torch.no_grad():
            feat = model.forward_features(img)["x_norm_clstoken"]
            sim = torch.nn.functional.cosine_similarity(query_feat, feat).item()
            results.append((path, sim))
    except:
        continue

# Sort by similarity
results.sort(key=lambda x: -x[1])

from IPython.display import display

print("üîç DINOv2 Top 10 Frame Matches:\n")
for i, (fname, score) in enumerate(results[:10]):
    print(f"{i+1}. {fname} ‚Üí Similarity: {score:.4f}")
    display(Image.open(fname))

import os
import requests
from bs4 import BeautifulSoup

chapter_url = "https://w58.sololevelingthemanga.com/manga/solo-leveling-chapter-9/"
chapter_url = "https://w58.sololevelingthemanga.com/manga/solo-leveling-chapter-6/"
save_dir = "solo_level_6"

os.makedirs(save_dir, exist_ok=True)
headers = {"User-Agent": "Mozilla/5.0"}

response = requests.get(chapter_url, headers=headers)
soup = BeautifulSoup(response.content, "html.parser")

main = soup.find("main")
article = main.find("article") if main else None
wrapper_divs = article.find_all("div") if article else []

img_links = []
for div in wrapper_divs:
    p_tags = div.find_all("p")
    for p in p_tags:
        img = p.find("img")
        if img and img.get("src"):
            img_links.append(img.get("src"))

for i, url in enumerate(img_links):
    try:
        img_data = requests.get(url, headers=headers).content
        filename = os.path.join(save_dir, f"page_{i+1:03}.jpg")
        with open(filename, "wb") as f:
            f.write(img_data)
        print(f"‚úÖ Saved: {filename}")
    except Exception as e:
        print(f"‚ùå Failed to download {url}: {e}")

print(f"\nüì¶ Done! {len(img_links)} images downloaded to: {save_dir}")

import os
import requests
from bs4 import BeautifulSoup
from PIL import Image
from io import BytesIO

# ‚úÖ Your working URL here
chapter_url = "https://readonepunch-man.com/read-one-punch-man-manga-chapter-17/"
save_dir = "one"

# Setup folder
os.makedirs(save_dir, exist_ok=True)
headers = {"User-Agent": "Mozilla/5.0"}

# Fetch HTML
response = requests.get(chapter_url, headers=headers)
soup = BeautifulSoup(response.content, "html.parser")

# ‚úÖ Find all <figure><img> under <article>
article = soup.find("article")
figures = article.find_all("figure") if article else []

img_links = []
for fig in figures:
    img = fig.find("img")
    if img and img.get("src"):
        img_links.append(img.get("src"))

# ‚úÖ Download & save all as .jpg
for i, url in enumerate(img_links):
    try:
        img_data = requests.get(url, headers=headers).content
        image = Image.open(BytesIO(img_data)).convert("RGB")
        filename = os.path.join(save_dir, f"page_{i+1:03}.jpg")
        image.save(filename, "JPEG")
        print(f"‚úÖ Saved: {filename}")
    except Exception as e:
        print(f"‚ùå Failed to process {url}: {e}")

print(f"\nüéâ Done! Downloaded {len(img_links)} images to /{save_dir}")

# !zip -r one17.zip one/
!zip -r solo_level6.zip solo_level_6/

import os
import torch
import clip
from PIL import Image, ImageDraw
import matplotlib.pyplot as plt
import numpy as np

# Load CLIP
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

def clip_match_and_show_grid(manga_folder, frame_folder, top_k=15, target_page=None):
    print("üîç Running CLIP similarity check...")

    # Get all file paths
    manga_images = sorted([os.path.join(manga_folder, f) for f in os.listdir(manga_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])
    frame_images = sorted([os.path.join(frame_folder, f) for f in os.listdir(frame_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])

    # Use target_page (single manga image) or all (default = first one)
    if target_page:
        manga_images = [target_page]
    else:
        manga_images = [manga_images[40]]  # just first for demo

    for manga_img_path in manga_images:
        print(f"\nüñºÔ∏è Matching manga page: {os.path.basename(manga_img_path)}")

        # Load manga image
        manga_img = preprocess(Image.open(manga_img_path).convert("RGB")).unsqueeze(0).to(device)
        with torch.no_grad():
            manga_feat = model.encode_image(manga_img)

        # Compare with all frames
        similarities = []
        for frame_path in frame_images:
            try:
                frame_img = preprocess(Image.open(frame_path).convert("RGB")).unsqueeze(0).to(device)
                with torch.no_grad():
                    frame_feat = model.encode_image(frame_img)
                score = torch.nn.functional.cosine_similarity(manga_feat, frame_feat).item()
                similarities.append((frame_path, score))
            except Exception as e:
                print(f"‚ö†Ô∏è Skipped {frame_path} due to error: {e}")

        # Sort by score
        top_matches = sorted(similarities, key=lambda x: -x[1])[:top_k]

        # Plot Grid
        plt.figure(figsize=(18, 2 * top_k))
        for i, (match_path, score) in enumerate(top_matches):
            manga_disp = Image.open(manga_img_path).resize((224, 224))
            frame_disp = Image.open(match_path).resize((224, 224))

            combined = Image.new('RGB', (448, 224))
            combined.paste(manga_disp, (0, 0))
            combined.paste(frame_disp, (224, 0))

            draw = ImageDraw.Draw(combined)
            draw.text((10, 5), f"Score: {score:.4f}", fill=(255, 255, 0))

            plt.subplot(top_k, 1, i + 1)
            plt.imshow(combined)
            plt.axis("off")
            plt.title(f"{os.path.basename(match_path)} | Similarity: {score:.4f}", fontsize=10)

        plt.tight_layout()
        plt.show()

clip_match_and_show_grid(
    manga_folder="/content/one",
    frame_folder="/content/frames",
    top_k=15
)

!rm -rf /content/solo_vid_frames
import os
os.makedirs("solo_vid_frames", exist_ok=True)

!ffmpeg -i solo_video.mp4 -vf fps=2 /content/solo_vid_frames/frame_%04d.png

clip_match_and_show_grid(
    manga_folder="/content/solo_level_6",
    frame_folder="/content/solo_vid_frames",
    top_k=60
)

import torch
# from dinov2.models import vision_transformer as vits
from torchvision import transforms
from PIL import Image

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load DINOv2 Base
# dino_model = vits.vit_base()
# state_dict = torch.hub.load_state_dict_from_url(
#     "https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14_pretrain.pth",
#     map_location=device
# )
# dino_model.load_state_dict(state_dict)
# dino_model.eval().to(device)
dinov2_vitb14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')
dino_model = dinov2_vitb14
dino_model.eval().cuda()
# DINO transform
dino_preprocess = transforms.Compose([
    transforms.Resize((518, 518)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])
import os
import matplotlib.pyplot as plt
from PIL import ImageDraw
from io import BytesIO

def dino_match_and_show_grid(manga_folder, frame_folder, top_k=15, target_page=None):
    print("üîç Running DINOv2 similarity check...")

    # Get all files
    manga_images = sorted([os.path.join(manga_folder, f) for f in os.listdir(manga_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])
    frame_images = sorted([os.path.join(frame_folder, f) for f in os.listdir(frame_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])

    # Use only 1 manga page for now
    if target_page:
        manga_images = [target_page]
    else:
        manga_images = [manga_images[40]]

    for manga_img_path in manga_images:
        print(f"\nüñºÔ∏è Matching manga page: {os.path.basename(manga_img_path)}")

        # Encode the panel
        panel_tensor = dino_preprocess(Image.open(manga_img_path).convert("RGB")).unsqueeze(0).to(device)
        with torch.no_grad():
            panel_feat = dino_model.forward_features(panel_tensor)["x_norm_clstoken"]

        similarities = []
        for frame_path in frame_images:
            try:
                frame_tensor = dino_preprocess(Image.open(frame_path).convert("RGB")).unsqueeze(0).to(device)
                with torch.no_grad():
                    frame_feat = dino_model.forward_features(frame_tensor)["x_norm_clstoken"]

                score = torch.nn.functional.cosine_similarity(panel_feat, frame_feat).item()
                similarities.append((frame_path, score))
            except Exception as e:
                print(f"‚ö†Ô∏è Skipped {frame_path} due to error: {e}")

        # Sort top matches
        top_matches = sorted(similarities, key=lambda x: -x[1])[:top_k]

        # Grid Display
        plt.figure(figsize=(18, 2 * top_k))
        for i, (match_path, score) in enumerate(top_matches):
            manga_disp = Image.open(manga_img_path).resize((224, 224))
            frame_disp = Image.open(match_path).resize((224, 224))

            combined = Image.new('RGB', (448, 224))
            combined.paste(manga_disp, (0, 0))
            combined.paste(frame_disp, (224, 0))

            draw = ImageDraw.Draw(combined)
            draw.text((10, 5), f"Score: {score:.4f}", fill=(255, 255, 0))

            plt.subplot(top_k, 1, i + 1)
            plt.imshow(combined)
            plt.axis("off")
            plt.title(f"{os.path.basename(match_path)} | Similarity: {score:.4f}", fontsize=10)

        plt.tight_layout()
        plt.show()

dino_match_and_show_grid(
    manga_folder="/content/solo_level_6",
    frame_folder="/content/solo_vid_frames",
    top_k=60
)

