# -*- coding: utf-8 -*-
"""transition generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hbji0Kn2IrT3a_r6SPLAgmpctq3sPOeP
"""

# Commented out IPython magic to ensure Python compatibility.
# Clone AnimateDiff repo
!git clone https://github.com/Kyotogod/AnimateDiff-Colab.git
# %cd AnimateDiff-Colab

# Install dependencies
!pip install -r requirements.txt

# Download AnimateDiff motion module (already included in notebook below, optional here)
!mkdir -p models/Motion_Module
!wget -P models/Motion_Module https://huggingface.co/guoyww/AnimateDiff/resolve/main/mm_sd_v15_v2.ckpt

# Download Stable Diffusion 1.5 model (anime one recommended)
!mkdir -p models/StableDiffusion
!wget -P models/StableDiffusion https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/model.ckpt

# Install ffmpeg if not already installed
!apt-get -y install ffmpeg

# Create the transition video
!ffmpeg -framerate 12 -i frames/frame_%03d.png -c:v libx264 -pix_fmt yuv420p transition.mp4

from PIL import Image
import os
start_img_path = "/content/panel.jpg"
end_img_path = "/content/end.png"
start_img = Image.open(start_img_path).resize((512, 512))
end_img = Image.open(end_img_path).resize((512, 512))

def interpolate_images(img1, img2, steps):
    img_seq = []
    for i in range(steps):
        alpha = i / (steps - 1)
        blended = Image.blend(img1, img2, alpha)
        img_seq.append(blended)
    return img_seq

frames = interpolate_images(start_img, end_img, steps=16)

# Save frames
os.makedirs("frames", exist_ok=True)
for i, frame in enumerate(frames):
    frame.save(f"frames/frame_{i:03d}.png")

!ffmpeg -framerate 12 -i frames/frame_%03d.png -c:v libx264 -pix_fmt yuv420p transition.mp4

!pip install diffusers transformers accelerate ftfy
!pip install git+https://github.com/openai/CLIP.git
!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

from diffusers import StableDiffusionPipeline
import torch
from PIL import Image
import numpy as np
import os

# Load SD1.5 from HuggingFace
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
).to("cuda")

pipe.enable_attention_slicing()

def image_to_latent(image_path):
    image = Image.open(image_path).convert("RGB").resize((512, 512))
    image = pipe.feature_extractor(images=image, return_tensors="pt").pixel_values.to("cuda", dtype=torch.float16)
    latents = pipe.vae.encode(image).latent_dist.sample() * 0.18215
    return latents

latent_start = image_to_latent("/content/panel.jpg")
latent_end = image_to_latent("/content/end.png")

def interpolate_latents(start, end, steps=16):
    return [(1 - t) * start + t * end for t in np.linspace(0, 1, steps)]

latent_seq = interpolate_latents(latent_start, latent_end, steps=16)

def latent_to_image(latent):
    latent = latent / 0.18215
    image = pipe.vae.decode(latent).sample
    image = (image / 2 + 0.5).clamp(0, 1)
    # image = image.cpu().permute(0, 2, 3, 1).numpy()[0]
    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()[0]

    return (image * 255).astype(np.uint8)

os.makedirs("transition_frames", exist_ok=True)

for i, latent in enumerate(latent_seq):
    img = latent_to_image(latent)
    Image.fromarray(img).save(f"transition_frames/frame_{i:03d}.png")

!ffmpeg -framerate 8 -i transition_frames/frame_%03d.png -c:v libx264 -pix_fmt yuv420p transition2.mp4

